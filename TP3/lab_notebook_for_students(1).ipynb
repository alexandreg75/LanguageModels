{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "682f35f1",
      "metadata": {
        "id": "682f35f1"
      },
      "source": [
        "# CSC 8614 - Language Models\n",
        "## CI3 - Parameter-Efficient Fine-Tuning with LoRA\n",
        "\n",
        "This TP builds upon the GPT architecture we have previously explored.\n",
        "\n",
        "We will implement Low-Rank Adaptation (LoRA) from scratch and inject it into our pre-existing GPTModel.\n",
        "\n",
        "Objectives:\n",
        "- Implement the mathematical formulation of LoRA.\n",
        "- Create a wrapper to convert standard Linear layers into LoRA-compatible layers.\n",
        "- Inject these layers into a pre-trained GPT model.\n",
        "- Verify that only a fraction of parameters are trainable.\n",
        "- Fine-tune the model with LoRA\n",
        "\n",
        "Some of this code comes from the book _Build a Large Language Model (From Scratch)_, by Sebastian Raschka, and its [official github repository](https://github.com/rasbt/LLMs-from-scratch).\n",
        "\n",
        "This TP will be done in this notebook, and requires some additional files (available from the course website).\n",
        "You will have to fill the missing portions of code, and perform some additional experiments by testing different parameters.\n",
        "\n",
        "Working on this TP:\n",
        "- The easiest way is probably to work directly on the notebook, using jupyter notebook or visual studio code. An alternative is also to use Google colab.\n",
        "- You should be able to run everything on your machine, but you can connect to the GPUs if needed.\n",
        "- **NOTE**: run the cells in the correct order, otherwise you might get errors due to inconsintencies.\n",
        "\n",
        "Some files are required, and are available on the course website and/or github repo:\n",
        "- `requirements.txt`\n",
        "- `gpt_utils.py`\n",
        "\n",
        "\n",
        "## About the report\n",
        "You will have to return this notebook (completed), as well as a mini-report (`TP3/rapport.md`).\n",
        "\n",
        "The notebook and report shall be submitted via a GitHub repository, similarly to what you did for the previous sessions (remember to use a different folder: `TP3`).\n",
        "For the notebook, it is sufficient to complete the code and submit the final version.\n",
        "\n",
        "For the mini-report, you have to answer the questions asked in this notebook, and discuss some of your findings as requested.\n",
        "Same as in the previous sessions:\n",
        "- You must include: short answers, observed results (copies of outputs), requested screenshots, and a brief interpretation.\n",
        "- Do not paste entire pages: be concise and select the relevant elements.\n",
        "\n",
        "Reproducibility:\n",
        "- fix a random seed and write it in the report\n",
        "- indicate in the report the specific python version OS, and the library versions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a8ead2",
      "metadata": {
        "id": "79a8ead2"
      },
      "source": [
        "## Prerequisite\n",
        "\n",
        "Install the requirements.\n",
        "\n",
        "**Note**: if you use the same virtual environment as last time, you will not have to reinstall everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "347af8dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "347af8dd",
        "outputId": "503ff41d-6cd7-41a0-a962-cdeb8fbd86b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.9.1 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: tiktoken==0.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
            "Collecting pandas==2.3.3 (from -r requirements.txt (line 4))\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.10.8 (from -r requirements.txt (line 5))\n",
            "  Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.20.0 (from -r requirements.txt (line 6))\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting jupyterlab==4.5.1 (from -r requirements.txt (line 7))\n",
            "  Downloading jupyterlab-4.5.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.3.20)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch==2.9.1->-r requirements.txt (line 1))\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken==0.12.0->-r requirements.txt (line 2)) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken==0.12.0->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (3.3.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (1.76.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (0.5.4)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
            "  Downloading async_lru-2.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.17.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.12/dist-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.9.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.14.0)\n",
            "Collecting jupyterlab-server<3,>=2.28.0 (from jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.4)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.5.1)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.12/dist-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.45.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (26.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.1->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (25.1.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.5.3)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.16.6)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.10.4)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.7.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.23.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.5.1)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
            "  Downloading json5-0.13.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.26.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken==0.12.0->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken==0.12.0->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (3.1.5)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (25.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.30.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.1.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.21.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (4.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.8.5)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (25.10.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.14)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.8.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.23)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.4.0)\n",
            "Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.5.1-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m139.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.1.0-py3-none-any.whl (6.9 kB)\n",
            "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.13.0-py3-none-any.whl (36 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, json5, jedi, async-lru, tensorboard, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, matplotlib, nvidia-cusolver-cu12, torch, tensorflow, jupyterlab-server, jupyter-lsp, jupyterlab\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.0\n",
            "    Uninstalling tensorflow-2.19.0:\n",
            "      Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed async-lru-2.1.0 jedi-0.19.2 json5-0.13.0 jupyter-lsp-2.3.0 jupyterlab-4.5.1 jupyterlab-server-2.28.0 matplotlib-3.10.8 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 tensorboard-2.20.0 tensorflow-2.20.0 torch-2.9.1 triton-3.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "a8e1e3d28294415a8f395561d1d694a4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed979fc7",
      "metadata": {
        "id": "ed979fc7"
      },
      "source": [
        "## Background & Mathematical Formulation\n",
        "\n",
        "Fine-tuning Large Language Models (LLMs) updates all model parameters, which is computationally expensive. LoRA freezes the pre-trained weights and injects trainable rank decomposition matrices.\n",
        "\n",
        "Given a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d_{out} \\times d_{in}}$, LoRA constrains the update $\\Delta W$ by representing it with a low-rank decomposition:\n",
        "\n",
        "$$W_0 + \\Delta W = W_0 + B A$$\n",
        "\n",
        "Where:\n",
        "- $B \\in \\mathbb{R}^{d_{out} \\times r}$\n",
        "- $A \\in \\mathbb{R}^{r \\times d_{in}}$\n",
        "- $r \\ll \\min(d_{in}, d_{out})$ is the rank.\n",
        "\n",
        "The Forward Pass:\n",
        "\n",
        "$$h = W_0 x + \\frac{\\alpha}{r} (B A x)$$\n",
        "\n",
        "- $\\alpha$ is a scaling constant.\n",
        "- $A$ is initialized with random Gaussian values.\n",
        "- $B$ is initialized with zeros (so training starts with no changes to the model)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecbec02b",
      "metadata": {
        "id": "ecbec02b"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the necessary libraries and our provided GPT utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b15a71ab",
      "metadata": {
        "id": "b15a71ab"
      },
      "outputs": [],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "from gpt_utils import GPTModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "685e3ab6",
      "metadata": {
        "id": "685e3ab6"
      },
      "outputs": [],
      "source": [
        "import random, numpy as np, torch\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2a222d",
      "metadata": {
        "id": "aa2a222d"
      },
      "source": [
        "## Implementing the LoRA Layer\n",
        "\n",
        "In this exercise, you will create the LoRALayer module. This module computes the $\\Delta Wx$ term (the branch on the right side of the diagram seen during the lecture).\n",
        "\n",
        "### **Exercise 1**: Define the LoRA Module\n",
        "\n",
        "Requirements:\n",
        "1. Define dimensions for parameters A and B based on in_dim, out_dim, and rank.\n",
        "2. Initialize A with kaiming_uniform_ (or small random normal).\n",
        "3. Initialize B with zeros.\n",
        "4. Implement the forward pass including the scaling factor $\\frac{\\alpha}{r}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "be539ccf",
      "metadata": {
        "id": "be539ccf"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank  # Scaling factor per LoRA paper\n",
        "\n",
        "        # A: (rank, in_dim) ; B: (out_dim, rank)\n",
        "        # Forward: x @ A.T -> (batch, rank), then @ B.T -> (batch, out_dim)\n",
        "        self.A = nn.Parameter(torch.empty(rank, in_dim))\n",
        "        self.B = nn.Parameter(torch.empty(out_dim, rank))\n",
        "\n",
        "        # Initializations\n",
        "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (..., in_dim)\n",
        "        # result: (..., out_dim)\n",
        "        result = (x @ self.A.t()) @ self.B.t()\n",
        "        return self.scaling * result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "lora = LoRALayer(in_dim=4, out_dim=3, rank=2, alpha=8)\n",
        "x = torch.randn(5, 4)\n",
        "y = lora(x)\n",
        "print(\"y shape:\", y.shape)        # (5, 3)\n",
        "print(\"y abs mean:\", y.abs().mean().item())  # devrait être 0.0 ou très proche\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVsmK9kNUTRc",
        "outputId": "317fde08-bc60-48c4-cb7b-e0804fdd3522"
      },
      "id": "sVsmK9kNUTRc",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y shape: torch.Size([5, 3])\n",
            "y abs mean: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0767c887",
      "metadata": {
        "id": "0767c887"
      },
      "source": [
        "## Wrapping Linear Layers\n",
        "\n",
        "We rarely replace the layer entirely; instead, we want a layer that holds both the frozen original weights and the new LoRA weights.\n",
        "\n",
        "### **Exercise 2**: The LinearWithLoRA Wrapper\n",
        "\n",
        "Requirements:\n",
        "1. Store the original linear layer.\n",
        "2. Create a self.lora instance using the dimensions of the original linear layer.\n",
        "3. In forward, add the output of the original layer to the output of the LoRA layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d3d268ad",
      "metadata": {
        "id": "d3d268ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1f95f7-379b-44d1-851c-d2330f945868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Passed: Wrapper acts identically to original layer at initialization.\n"
          ]
        }
      ],
      "source": [
        "class LinearWithLoRA(nn.Module):\n",
        "    def __init__(self, linear_layer, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear_layer\n",
        "\n",
        "        # Instantiate LoRALayer with same in/out dims as the original linear layer\n",
        "        self.lora = LoRALayer(\n",
        "            in_dim=linear_layer.in_features,\n",
        "            out_dim=linear_layer.out_features,\n",
        "            rank=rank,\n",
        "            alpha=alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Combine frozen (original) + LoRA adaptation\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n",
        "\n",
        "input_x = torch.randn(1, 128)\n",
        "original_layer = nn.Linear(128, 64)\n",
        "lora_wrapped = LinearWithLoRA(original_layer, rank=4, alpha=8)\n",
        "\n",
        "assert torch.allclose(original_layer(input_x), lora_wrapped(input_x))\n",
        "print(\"Test Passed: Wrapper acts identically to original layer at initialization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "201146a1",
      "metadata": {
        "id": "201146a1"
      },
      "source": [
        "## Injecting LoRA into GPTModel\n",
        "\n",
        "Now we need to modify our existing GPTModel. We cannot manually rewrite the class. Instead, we will iterate through the model's modules and replace specific layers dynamically.\n",
        "\n",
        "In GPTModel, the transformer blocks are stored in *self.trf_blocks*. Inside those, we have attention mechanisms (att) containing W_query, W_key, W_value, or a combined c_attn.\n",
        "\n",
        "Note: For this lab, to keep it simple, we will replace all nn.Linear layers except the final output head.\n",
        "\n",
        "### **Exercise 3**: Recursive Model Modification\n",
        "\n",
        "Requirements:\n",
        "1. Iterate through named children of the model.\n",
        "2. If a module is nn.Linear, wrap it in LinearWithLoRA.\n",
        "3. Important: Skip the final output layer (often named out_head or similar in gpt_utils), as we usually don't want to reduce the rank of the vocabulary projection."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "from gpt_utils import GPTModel, download_and_load_gpt2, load_weights_into_gpt\n",
        "\n",
        "# Re-download/load weights if needed\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2_weights\")\n",
        "\n",
        "model_config = {\n",
        "    \"vocab_size\": settings[\"n_vocab\"],\n",
        "    \"context_length\": settings[\"n_ctx\"],\n",
        "    \"emb_dim\": settings[\"n_embd\"],\n",
        "    \"n_heads\": settings[\"n_head\"],\n",
        "    \"n_layers\": settings[\"n_layer\"],\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": True,\n",
        "}\n",
        "\n",
        "model = GPTModel(model_config)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model reloaded ✅\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3HDIQVfXiMa",
        "outputId": "95e95f86-9166-4143-cd29-8b41ff990d37"
      },
      "id": "j3HDIQVfXiMa",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 131kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.56MiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 172kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:29<00:00, 17.1MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 7.93MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.58MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.42MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model reloaded ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "116d7337",
      "metadata": {
        "id": "116d7337"
      },
      "outputs": [],
      "source": [
        "def replace_linear_with_lora(model, rank, alpha):\n",
        "    \"\"\"\n",
        "    Recursively replaces nn.Linear with LinearWithLoRA,\n",
        "    skipping the final output head (out_head).\n",
        "    \"\"\"\n",
        "    for name, module in model.named_children():\n",
        "\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Skip final output projection head\n",
        "            if name == \"out_head\":\n",
        "                continue\n",
        "\n",
        "            # Wrap current Linear layer\n",
        "            new_layer = LinearWithLoRA(module, rank=rank, alpha=alpha)\n",
        "\n",
        "            # Replace it in the parent module\n",
        "            setattr(model, name, new_layer)\n",
        "\n",
        "        else:\n",
        "            # Recurse into submodules\n",
        "            replace_linear_with_lora(module, rank, alpha)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple: injecter LoRA\n",
        "rank = 4\n",
        "alpha = 8\n",
        "replace_linear_with_lora(model, rank=rank, alpha=alpha)\n",
        "\n",
        "# Vérifier que out_head n'est pas wrappé\n",
        "print(\"out_head type:\", type(model.out_head))\n",
        "\n",
        "# Chercher un LinearWithLoRA quelque part\n",
        "found = False\n",
        "for n, m in model.named_modules():\n",
        "    if isinstance(m, LinearWithLoRA):\n",
        "        print(\"Found LoRA layer at:\", n)\n",
        "        found = True\n",
        "        break\n",
        "print(\"Any LoRA layers found?\", found)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4ItsU8XXQdI",
        "outputId": "01a28f10-a0c7-46d1-8613-ce8df8a07ce7"
      },
      "id": "K4ItsU8XXQdI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_head type: <class 'torch.nn.modules.linear.Linear'>\n",
            "Found LoRA layer at: trf_blocks.0.att.W_query\n",
            "Any LoRA layers found? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d5f1d3",
      "metadata": {
        "id": "c2d5f1d3"
      },
      "source": [
        "## Freeze & Verify\n",
        "\n",
        "We have injected the layers, but currently, everything is still trainable. We must freeze the original weights.\n",
        "\n",
        "### **Exercise 4**: Freezing and Counting Parameters\n",
        "Requirements:\n",
        "1. Set requires_grad = False for all parameters.\n",
        "2. Iterate through the model; if a layer is LinearWithLoRA, unfreeze self.lora.A and self.lora.B.\n",
        "3. Calculate the ratio of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5764f658",
      "metadata": {
        "id": "5764f658"
      },
      "outputs": [],
      "source": [
        "def freeze_and_activate_lora(model):\n",
        "    # 1) Freeze ALL parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 2) Unfreeze only LoRA A and B matrices\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, LinearWithLoRA):\n",
        "            module.lora.A.requires_grad = True\n",
        "            module.lora.B.requires_grad = True\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before freezing:\")\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "freeze_and_activate_lora(model)\n",
        "\n",
        "print(\"\\nAfter freezing + LoRA activation:\")\n",
        "print_trainable_parameters(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXZtpRzYZhtq",
        "outputId": "69d29f73-9f82-479c-c0da-c5ba2eb9b4aa"
      },
      "id": "CXZtpRzYZhtq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before freezing:\n",
            "trainable params: 163,700,736 || all params: 163,700,736 || trainable%: 100.00%\n",
            "\n",
            "After freezing + LoRA activation:\n",
            "trainable params: 663,552 || all params: 163,700,736 || trainable%: 0.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0ab6916",
      "metadata": {
        "id": "d0ab6916"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "Let's download and initialize the model from gpt_utils, then apply our LoRA transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "73aae7d7",
      "metadata": {
        "id": "73aae7d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ad875e-8539-4167-ee0f-9d65fa304d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2_weights/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2_weights/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2_weights/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2_weights/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2_weights/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2_weights/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2_weights/124M/vocab.bpe\n",
            "Weights downloaded and loaded into memory.\n"
          ]
        }
      ],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "# (Basically same code as last session)\n",
        "\n",
        "from gpt_utils import GPTModel, download_and_load_gpt2, load_weights_into_gpt\n",
        "\n",
        "# Download the model weights (124M param version), and initialize it.\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2_weights\")\n",
        "print(\"Weights downloaded and loaded into memory.\")\n",
        "\n",
        "# Configure the model, mapping OpenAI specific keys to our model's keys (if needed)\n",
        "model_config = {\n",
        "    \"vocab_size\": settings[\"n_vocab\"],\n",
        "    \"context_length\": settings[\"n_ctx\"],\n",
        "    \"emb_dim\": settings[\"n_embd\"],\n",
        "    \"n_heads\": settings[\"n_head\"],\n",
        "    \"n_layers\": settings[\"n_layer\"],\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": True,\n",
        "}\n",
        "\n",
        "# Initialize the Base Model\n",
        "model = GPTModel(model_config)\n",
        "load_weights_into_gpt(model, params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c26ffba",
      "metadata": {
        "id": "2c26ffba"
      },
      "source": [
        "Now, we call all the methods we have defined above, and put everything together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4001e300",
      "metadata": {
        "id": "4001e300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2f704d-1612-4aa5-90fb-ed85e314dcf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Structure (Truncated):\n",
            "TransformerBlock(\n",
            "  (att): MultiHeadAttention(\n",
            "    (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (ff): FeedForward(\n",
            "    (layers): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "      (1): GELU()\n",
            "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (norm1): LayerNorm()\n",
            "  (norm2): LayerNorm()\n",
            "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "\n",
            "Model Structure After LoRA (Truncated):\n",
            "TransformerBlock(\n",
            "  (att): MultiHeadAttention(\n",
            "    (W_query): LinearWithLoRA(\n",
            "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (W_key): LinearWithLoRA(\n",
            "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (W_value): LinearWithLoRA(\n",
            "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (out_proj): LinearWithLoRA(\n",
            "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (ff): FeedForward(\n",
            "    (layers): Sequential(\n",
            "      (0): LinearWithLoRA(\n",
            "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (lora): LoRALayer()\n",
            "      )\n",
            "      (1): GELU()\n",
            "      (2): LinearWithLoRA(\n",
            "        (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (lora): LoRALayer()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm1): LayerNorm()\n",
            "  (norm2): LayerNorm()\n",
            "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "\n",
            "Parameter Count:\n",
            "trainable params: 1,327,104 || all params: 164,364,288 || trainable%: 0.81%\n"
          ]
        }
      ],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "print(\"Original Model Structure (Truncated):\")\n",
        "print(model.trf_blocks[0]) # Print first block to see standard Linear layers\n",
        "\n",
        "# Apply LoRA Replacement\n",
        "# Rank 8, Alpha 16 (Alpha is usually set to 2x Rank as a rule of thumb)\n",
        "replace_linear_with_lora(model, rank=8, alpha=16)\n",
        "\n",
        "# 3. Freeze Weights\n",
        "freeze_and_activate_lora(model)\n",
        "\n",
        "# 4. Check Results\n",
        "print(\"\\nModel Structure After LoRA (Truncated):\")\n",
        "print(model.trf_blocks[0])\n",
        "\n",
        "print(\"\\nParameter Count:\")\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a264553",
      "metadata": {
        "id": "1a264553"
      },
      "source": [
        "**Question 1:** Do you see any difference between \"Original Model Structure (Truncated)\" and \"Model Structure After LoRA (Truncated)\"? Do you see the LinearWithLoRA you have defined above?\n",
        "\n",
        "**Question 2:** What is the number of trainable parameters, all parameters, and the fraction of trainable parameters?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd2e97c6",
      "metadata": {
        "id": "cd2e97c6"
      },
      "source": [
        "## Training Loop Verification\n",
        "\n",
        "Finally, let's prove that gradients are only generated for the specific LoRA parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8076639f",
      "metadata": {
        "id": "8076639f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43d5648-6703-4650-fef8-30fc63009dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradient Verification:\n",
            "Parameters with gradients: 144\n",
            "Frozen parameters correctly without gradients: 197\n",
            "SUCCESS: Gradients are flowing correctly only into LoRA parameters.\n"
          ]
        }
      ],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "# Create dummy input\n",
        "batch_size = 2\n",
        "dummy_input = torch.randint(0, 1000, (batch_size, 256))\n",
        "dummy_target = torch.randint(0, 1000, (batch_size, 256))\n",
        "\n",
        "# Optimizer setup\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=1e-3)\n",
        "\n",
        "# Forward Pass\n",
        "logits = model(dummy_input)\n",
        "loss = torch.nn.functional.cross_entropy(logits.view(-1, model_config['vocab_size']), dummy_target.view(-1))\n",
        "\n",
        "# Backward Pass\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Verification Step\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\nGradient Verification:\")\n",
        "grads_found = 0\n",
        "grads_missing = 0\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        if param.grad is not None:\n",
        "            grads_found += 1\n",
        "        else:\n",
        "            print(f\"WARNING: Trainable parameter {name} has no gradient.\")\n",
        "    else:\n",
        "        if param.grad is not None:\n",
        "            print(f\"ERROR: Frozen parameter {name} has a gradient!\")\n",
        "        else:\n",
        "            grads_missing += 1\n",
        "\n",
        "print(f\"Parameters with gradients: {grads_found}\")\n",
        "print(f\"Frozen parameters correctly without gradients: {grads_missing}\")\n",
        "\n",
        "if grads_found > 0 and grads_missing > 0:\n",
        "    print(\"SUCCESS: Gradients are flowing correctly only into LoRA parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff671a8",
      "metadata": {
        "id": "0ff671a8"
      },
      "source": [
        "## SPAM Classification\n",
        "\n",
        "Let's now work the Spam Classification task again, but this time using the LoRA-adapted model.\n",
        "This follows the example from the previous session."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2933e138",
      "metadata": {
        "id": "2933e138"
      },
      "source": [
        "### Download and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "xOulidqsd7bH"
      },
      "id": "xOulidqsd7bH",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8beff640",
      "metadata": {
        "id": "8beff640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7cb07b-e1bd-46c0-ff26-7999697e41d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Dataset downloaded and extracted.\n",
            "Loaded 5572 examples.\n",
            "  label                                               text\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "Balanced Dataset size: 1494\n",
            "Size of train_df: 1195\n",
            "Size of test_df: 299\n",
            "Distribution of labels in the dataset:\n",
            "train:\n",
            "label\n",
            "1    612\n",
            "0    583\n",
            "Name: count, dtype: int64\n",
            "test:\n",
            "label\n",
            "0    164\n",
            "1    135\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Download the dataset\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = os.path.join(extracted_path, \"SMSSpamCollection\")\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "    print(\"Dataset downloaded and extracted.\")\n",
        "\n",
        "# Load into DataFrame\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
        "print(f\"Loaded {len(df)} examples.\")\n",
        "print(df.head())\n",
        "\n",
        "# Class Balancing (primarily for speed in the lab)\n",
        "spam_df = df[df[\"label\"] == \"spam\"]\n",
        "ham_df = df[df[\"label\"] == \"ham\"].sample(len(spam_df), random_state=RANDOM_STATE)\n",
        "df = pd.concat([spam_df, ham_df]).reset_index(drop=True)\n",
        "\n",
        "# Map labels to integers\n",
        "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "print(f\"Balanced Dataset size: {len(df)}\")\n",
        "\n",
        "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "train_size = int(0.8 * len(df))\n",
        "train_df = df[:train_size]\n",
        "test_df = df[train_size:]\n",
        "\n",
        "print(f\"Size of train_df: {len(train_df)}\")\n",
        "print(f\"Size of test_df: {len(test_df)}\")\n",
        "\n",
        "print(\"Distribution of labels in the dataset:\")\n",
        "print(\"train:\")\n",
        "print(train_df['label'].value_counts())\n",
        "print(\"test:\")\n",
        "print(test_df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fb81f48",
      "metadata": {
        "id": "6fb81f48"
      },
      "source": [
        "Define Dataset and DataLoader, similarly to previous session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f7b33971",
      "metadata": {
        "id": "f7b33971"
      },
      "outputs": [],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "import tiktoken\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        text = row['text']\n",
        "        label = row['label']\n",
        "\n",
        "        # Tokenize\n",
        "        encoded = self.tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "\n",
        "        # Truncate\n",
        "        encoded = encoded[:self.max_length]\n",
        "\n",
        "        # Pad (GPT-2 usually uses <|endoftext|> as padding)\n",
        "        pad_len = self.max_length - len(encoded)\n",
        "        encoded = encoded + [50256] * pad_len # 50256 is <|endoftext|> in GPT2\n",
        "\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Initialize Tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Create Loaders\n",
        "train_loader = DataLoader(SpamDataset(train_df, tokenizer), batch_size=8, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(SpamDataset(test_df, tokenizer), batch_size=8, shuffle=False, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9b9dc8",
      "metadata": {
        "id": "2f9b9dc8"
      },
      "source": [
        "We now modify the Model for Classification, replacing the final layer (out_head)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3e686056",
      "metadata": {
        "id": "3e686056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5200e9-014f-4fb4-a6cc-58fa46abe778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Output Head: Linear(in_features=768, out_features=2, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "# Check input dimension of the current head\n",
        "hidden_dim = model.out_head.in_features\n",
        "\n",
        "# Replace the head\n",
        "model.out_head = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "# Move model to device (if using GPU, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"New Output Head:\", model.out_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f4b703",
      "metadata": {
        "id": "e1f4b703"
      },
      "source": [
        "We previously froze everything except LoRA. Now we added a new head, let's make it unfrozen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6889edeb",
      "metadata": {
        "id": "6889edeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4cf6f7-bbbd-4a51-b436-f9e686edc70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,328,642 || all params: 125,768,450 || trainable%: 1.06%\n"
          ]
        }
      ],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "def set_classification_trainable(model):\n",
        "    # Ensure LoRA layers are trainable (A and B)\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, LinearWithLoRA):\n",
        "            module.lora.A.requires_grad = True\n",
        "            module.lora.B.requires_grad = True\n",
        "\n",
        "    # Ensure the new classification head is trainable\n",
        "    for param in model.out_head.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "set_classification_trainable(model)\n",
        "\n",
        "# Verify count again\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96dec47a",
      "metadata": {
        "id": "96dec47a"
      },
      "source": [
        "**Question 3:** Check the number (and fraction) of trainable parameters, and compare it with the one above. Do you see any differences? Can you describe them?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36a5f0ac",
      "metadata": {
        "id": "36a5f0ac"
      },
      "source": [
        "The Training Loop\n",
        "Context: Standard PyTorch loop. Note that GPT models output [batch, seq_len, hidden]. For classification, we usually take the hidden state of the last token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c03d3f27",
      "metadata": {
        "id": "c03d3f27"
      },
      "outputs": [],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "import time\n",
        "\n",
        "def train_classifier(model, loader, optimizer, device, epochs=1):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward Pass\n",
        "            # The model outputs (batch, seq_len, num_classes)\n",
        "            logits = model(inputs)\n",
        "\n",
        "            # Select the last token for classification\n",
        "            # last_token_logits = logits[:, -1, :]\n",
        "            # NOTE: this (the line above) was an error in the code provided with the previous lab: it was using the last token (which is most often PAD),\n",
        "            #   not the last non padding token.\n",
        "            # Select the last NON-PADDING token\n",
        "            #   Create a mask (1 for real tokens, 0 for PAD); 50256 is the PAD token ID\n",
        "            mask = (inputs != 50256)\n",
        "\n",
        "            # Find the index of the last real token\n",
        "            #    Summing the mask gives the length. Subtract 1 for 0-based index.\n",
        "            #    .clamp(min=0) prevents errors if a sequence is empty (unlikely)\n",
        "            last_idx = (mask.sum(dim=1) - 1).clamp(min=0)\n",
        "\n",
        "            # Select the logits at those specific indices\n",
        "            #    We use torch.arange for the batch dimension\n",
        "            batch_indices = torch.arange(inputs.size(0), device=device)\n",
        "            last_token_logits = logits[batch_indices, last_idx, :]\n",
        "\n",
        "            loss = torch.nn.functional.cross_entropy(last_token_logits, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy for monitoring\n",
        "            preds = torch.argmax(last_token_logits, dim=-1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1} | Batch {batch_idx} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        acc = correct / total * 100\n",
        "        print(f\"Epoch {epoch+1} Finished | Avg Loss: {total_loss/len(loader):.4f} | Acc: {acc:.2f}% | Time: {time.time()-start_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5d347648",
      "metadata": {
        "id": "5d347648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d91ce99-ee1f-429c-fc73-f807765eb1cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Batch 0 | Loss: 8.4209\n",
            "Epoch 1 | Batch 10 | Loss: 0.1267\n",
            "Epoch 1 | Batch 20 | Loss: 0.2919\n",
            "Epoch 1 | Batch 30 | Loss: 0.0225\n",
            "Epoch 1 | Batch 40 | Loss: 0.0425\n",
            "Epoch 1 | Batch 50 | Loss: 0.6400\n",
            "Epoch 1 | Batch 60 | Loss: 0.0400\n",
            "Epoch 1 | Batch 70 | Loss: 0.0758\n",
            "Epoch 1 | Batch 80 | Loss: 0.0169\n",
            "Epoch 1 | Batch 90 | Loss: 0.0063\n",
            "Epoch 1 | Batch 100 | Loss: 0.0047\n",
            "Epoch 1 | Batch 110 | Loss: 0.0053\n",
            "Epoch 1 | Batch 120 | Loss: 0.1009\n",
            "Epoch 1 | Batch 130 | Loss: 0.0234\n",
            "Epoch 1 | Batch 140 | Loss: 0.0106\n",
            "Epoch 1 Finished | Avg Loss: 0.3766 | Acc: 92.03% | Time: 51.88s\n"
          ]
        }
      ],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "# Setup Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=5e-4    # TODO: Potentially test with different learning rates\n",
        ")\n",
        "\n",
        "# Run Training\n",
        "train_classifier(model, train_loader, optimizer, device, epochs=1)  # TODO: Potentially test with different numbers of epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02917255",
      "metadata": {
        "id": "02917255"
      },
      "source": [
        "**Question 4:** Can you describe the trend of the loss, and the final accuracy. Is it reasonable considering the task at hand?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32e655eb",
      "metadata": {
        "id": "32e655eb"
      },
      "source": [
        "We can now test the accuracy on the held-out test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fd866e52",
      "metadata": {
        "id": "fd866e52"
      },
      "outputs": [],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "def evaluate_accuracy(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(inputs)\n",
        "\n",
        "            # Select last \"real\" token logits\n",
        "            mask = (inputs != 50256)\n",
        "            last_idx = (mask.sum(dim=1) - 1).clamp(min=0)\n",
        "            batch_idx = torch.arange(inputs.size(0), device=device)\n",
        "            last_token_logits = logits[batch_idx, last_idx, :]\n",
        "            # Select last token logits (same logic as training)\n",
        "            # last_token_logits = logits[:, -1, :]\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = torch.argmax(last_token_logits, dim=-1)\n",
        "\n",
        "            # Compare with targets\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "553c3a54",
      "metadata": {
        "id": "553c3a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ff377a-5a13-4947-ffce-12b670615032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Accuracy: 97.99%\n"
          ]
        }
      ],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "# Run evaluation\n",
        "test_accuracy = evaluate_accuracy(model, test_loader, device)\n",
        "print(f\"Test Set Accuracy: {test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c713ca7",
      "metadata": {
        "id": "8c713ca7"
      },
      "source": [
        "**Question 5:** How is the accuracy, and how does it compare to the Train set accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c1b259",
      "metadata": {
        "id": "43c1b259"
      },
      "source": [
        "Finally, we can do a quick inference test, to see how the model classifies new texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f06b8950",
      "metadata": {
        "id": "f06b8950"
      },
      "outputs": [],
      "source": [
        "# --- INSTRUCTOR CODE ---\n",
        "\n",
        "def classify_text(text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded = encoded[:256] # Truncate\n",
        "    tensor_input = torch.tensor([encoded], dtype=torch.long).to(device) # Add batch dim\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(tensor_input)\n",
        "        last_token_logits = logits[:, -1, :]\n",
        "        probs = torch.softmax(last_token_logits, dim=-1)\n",
        "        pred_label = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "    label_map = {0: \"HAM (Normal)\", 1: \"SPAM\"}\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Prediction: {label_map[pred_label]} (Confidence: {probs[0][pred_label]:.2f})\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d358eedb",
      "metadata": {
        "id": "d358eedb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e58003e-1cbd-4a2f-97d0-eb7f185cda6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'There is a big cash prize for you, call immediately.'\n",
            "Prediction: HAM (Normal) (Confidence: 0.67)\n",
            "------------------------------\n",
            "Text: 'Hey, are we still meeting for lunch tomorrow?'\n",
            "Prediction: HAM (Normal) (Confidence: 1.00)\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# TODO: add the text you want to test.\n",
        "classify_text(\"There is a big cash prize for you, call immediately.\", model, tokenizer, device)\n",
        "classify_text(\"Hey, are we still meeting for lunch tomorrow?\", model, tokenizer, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b4d416",
      "metadata": {
        "id": "28b4d416"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}